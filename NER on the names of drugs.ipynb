{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Project2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbEZhX3_BYjn",
        "outputId": "32d44116-908b-4a54-994a-3e267be40e4a"
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvnxTMyWBq1Y"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import spacy \n",
        "import en_core_web_sm \n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.parse import CoreNLPParser\n",
        "import json\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus.reader.conll import ConllCorpusReader\n",
        "from keras.layers import merge, TimeDistributed, Dropout, Bidirectional, Flatten, Masking\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as f_score\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYOMvN1IBvPq"
      },
      "source": [
        "train = pd.read_csv(\"train.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "test = pd.read_csv(\"test.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqAl4uCYByt2",
        "outputId": "f8446b65-035d-456b-9f70-f72d0f03da35"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1316462, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKF-mNQfB1go",
        "outputId": "e3f9ae0c-f963-4431-d57e-a9b9bb99bc32"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1272841, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHTeSpIMB5SJ"
      },
      "source": [
        "train.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoiaN7aoB7zv",
        "outputId": "6cb9f855-f33c-4450-a11f-de69bd400521"
      },
      "source": [
        "test.columns.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['id,Doc_ID,Sent_ID,Word'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IY_DHCfB-o8"
      },
      "source": [
        "df = train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY6PCx7uDOan",
        "outputId": "e9d5afba-5470-4175-e80d-d0f9de7c1219"
      },
      "source": [
        "print(df[:700])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id  Doc_ID  Sent_ID               Word tag\n",
            "0      1     1.0      1.0            Obesity   O\n",
            "1      2     1.0      1.0                 in   O\n",
            "2      3     1.0      1.0               Low-   O\n",
            "3      4     1.0      1.0                and   O\n",
            "4      5     1.0      1.0      Middle-Income   O\n",
            "..   ...     ...      ...                ...  ..\n",
            "695  696     5.0     32.0                  h   O\n",
            "696  697     5.0     32.0  post-transfection   O\n",
            "697  698     5.0     32.0                  .   O\n",
            "698  699     5.0     33.0                 By   O\n",
            "699  700     5.0     33.0                 72   O\n",
            "\n",
            "[700 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2T9oigECGUG"
      },
      "source": [
        "df1 = test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KMkl_gODU_D",
        "outputId": "5abb175b-8184-4ce5-8414-174e32f5bc32"
      },
      "source": [
        "print(df1[:700])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id   Doc_ID   Sent_ID    Word\n",
            "0    4543834  30001.0  191283.0   CCCVA\n",
            "1    4543835  30001.0  191283.0       ,\n",
            "2    4543836  30001.0  191283.0  MANOVA\n",
            "3    4543837  30001.0  191283.0       ,\n",
            "4    4543838  30001.0  191283.0      my\n",
            "..       ...      ...       ...     ...\n",
            "695  4544529  30009.0  191317.0    Most\n",
            "696  4544530  30009.0  191317.0       (\n",
            "697  4544531  30009.0  191317.0    58.1\n",
            "698  4544532  30009.0  191317.0       %\n",
            "699  4544533  30009.0  191317.0       )\n",
            "\n",
            "[700 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giueXX2bCQQP",
        "outputId": "5061dd8e-2557-438f-fe99-7ed922df1c5b"
      },
      "source": [
        "# Convert .tsv file to dataturks json format. \n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
        "    try:\n",
        "        f=open(input_path,'r') # input file\n",
        "        fp=open(output_path, 'w') # output file\n",
        "        data_dict={}\n",
        "        annotations =[]\n",
        "        label_dict={}\n",
        "        s=''\n",
        "        start=0\n",
        "        for line in f:\n",
        "            if line[0:len(line)-1]!='.\\tO':\n",
        "                word,entity=line.split('\\t')\n",
        "                s+=word+\" \"\n",
        "                entity=entity[:len(entity)-1]\n",
        "                if entity!=unknown_label:\n",
        "                    if len(entity) != 1:\n",
        "                        d={}\n",
        "                        d['text']=word\n",
        "                        d['start']=start\n",
        "                        d['end']=start+len(word)-1  \n",
        "                        try:\n",
        "                            label_dict[entity].append(d)\n",
        "                        except:\n",
        "                            label_dict[entity]=[]\n",
        "                            label_dict[entity].append(d) \n",
        "                start+=len(word)+1\n",
        "            else:\n",
        "                data_dict['content']=s\n",
        "                s=''\n",
        "                label_list=[]\n",
        "                for ents in list(label_dict.keys()):\n",
        "                    for i in range(len(label_dict[ents])):\n",
        "                        if(label_dict[ents][i]['text']!=''):\n",
        "                            l=[ents,label_dict[ents][i]]\n",
        "                            for j in range(i+1,len(label_dict[ents])): \n",
        "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
        "                                    di={}\n",
        "                                    di['start']=label_dict[ents][j]['start']\n",
        "                                    di['end']=label_dict[ents][j]['end']\n",
        "                                    di['text']=label_dict[ents][i]['text']\n",
        "                                    l.append(di)\n",
        "                                    label_dict[ents][j]['text']=''\n",
        "                            label_list.append(l)                          \n",
        "                            \n",
        "                for entities in label_list:\n",
        "                    label={}\n",
        "                    label['label']=[entities[0]]\n",
        "                    label['points']=entities[1:]\n",
        "                    annotations.append(label)\n",
        "                data_dict['annotation']=annotations\n",
        "                annotations=[]\n",
        "                json.dump(data_dict, fp)\n",
        "                fp.write('\\n')\n",
        "                data_dict={}\n",
        "                start=0\n",
        "                label_dict={}\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "tsv_to_json_format(\"train.tsv\",'train.json','abc')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Unable to process file\n",
            "error = [Errno 2] No such file or directory: 'Data/ner_corpus_260.tsv'\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-0aa3b8b9c7a1>\", line 7, in tsv_to_json_format\n",
            "    f=open(input_path,'r') # input file\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'Data/ner_corpus_260.tsv'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bDPsYZMOgFg"
      },
      "source": [
        "pip install –U spacy\n",
        "python –m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFFn7LmBCaoL"
      },
      "source": [
        "spacy.prefer_gpu()\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGXaDaZwVxEg"
      },
      "source": [
        "def train_spacy(data,iterations):\n",
        "    TRAIN_DATA = data\n",
        "    nlp = spacy.blank('en')  \n",
        "\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "       \n",
        "\n",
        "\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  \n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(iterations):\n",
        "            print(\"Statring iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  \n",
        "                    [annotations],  \n",
        "                    drop=0.2,  \n",
        "                    sgd=optimizer,  \n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    return nlp\n",
        "\n",
        "\n",
        "prdnlp = train_spacy(TRAIN_DATA, 20)\n",
        "\n",
        "modelfile = input(\"Enter your Model Name: \")\n",
        "prdnlp.to_disk(modelfile)\n",
        "\n",
        "\n",
        "tp=0\n",
        "tr=0\n",
        "tf=0\n",
        "\n",
        "ta=0\n",
        "c=0    \n",
        "\n",
        "for text,annot in df1:\n",
        "\n",
        "    doc_to_test=prdnlp(text)\n",
        "\n",
        "    d={}\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_]=[]\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_].append(ent.text)\n",
        "\n",
        "    d={}\n",
        "\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_]=[0,0,0,0,0,0]\n",
        "    for ent in doc_to_test.ents:\n",
        "        doc_gold_text= prdnlp.make_doc(text)\n",
        "        gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
        "        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
        "        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
        "        if(d[ent.label_][0]==0):\n",
        "            print(\"For Entity \"+ent.label_+\"\\n\")   \n",
        "            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
        "            a=accuracy_score(y_true,y_pred)\n",
        "            d[ent.label_][0]=1\n",
        "            d[ent.label_][1]+=p\n",
        "            d[ent.label_][2]+=r\n",
        "            d[ent.label_][3]+=f\n",
        "            d[ent.label_][4]+=a\n",
        "            d[ent.label_][5]+=1\n",
        "    c+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UIkL4p_ECvz"
      },
      "source": [
        "words = data[\"Word\"].values.tolist()\n",
        "tags = data[\"tag\"].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m8jIW-KWP9l"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(word, tag)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr-pPsxsGNr8",
        "outputId": "7ad7d649-1981-47e8-a295-e374e216480d"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model-piyush.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} ')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch: 01 \n",
            "Train Loss: 4.212\n",
            "Val. Loss: 4.096 \n",
            "Epoch: 02\n",
            "Train Loss: 3.767\n",
            "Val. Loss: 3.942 \n",
            "Epoch: 03\n",
            "Train Loss: 3.506\n",
            "Val. Loss: 4.049\n",
            "Epoch: 04\n",
            "Train Loss: 3.351\n",
            "Val. Loss: 3.907\n",
            "Epoch: 05\n",
            "Train Loss: 3.248\n",
            "Val. Loss: 3.860\n",
            "Epoch: 06\n",
            "Train Loss: 3.144\n",
            "Val. Loss: 3.795\n",
            "Epoch: 07\n",
            "Train Loss: 3.008\n",
            "Val. Loss: 3.660\n",
            "Epoch: 08\n",
            "Train Loss: 2.856\n",
            "Val. Loss: 3.582\n",
            "Epoch: 09\n",
            "Train Loss: 2.705\n",
            "Val. Loss: 3.538\n",
            "Epoch: 10\n",
            "Train Loss: 2.559\n",
            "Val. Loss: 3.397\n",
            "Epoch: 11\n",
            "Train Loss: 2.428\n",
            "Val. Loss: 3.331\n",
            "Epoch: 12\n",
            "Train Loss: 2.275\n",
            "Val. Loss: 3.290\n",
            "Epoch: 13\n",
            "Train Loss: 2.162\n",
            "Val. Loss: 3.158\n",
            "Epoch: 14\n",
            "Train Loss: 2.038\n",
            "Val. Loss: 3.117\n",
            "Epoch: 15\n",
            "Train Loss: 1.947\n",
            "Val. Loss: 3.091\n",
            "Epoch: 16\n",
            "Train Loss: 1.850\n",
            "Val. Loss: 3.062\n",
            "Epoch: 17\n",
            "Train Loss: 1.756\n",
            "Val. Loss: 3.022\n",
            "Epoch: 18 \n",
            "Train Loss: 1.622\n",
            "Val. Loss: 3.085 \n",
            "Epoch: 19\n",
            "Train Loss: 1.554\n",
            "Val. Loss: 3.003\n",
            "Epoch: 20\n",
            "Train Loss: 1.480\n",
            "Val. Loss: 3.043\n",
            "Epoch: 21\n",
            "Train Loss: 1.382\n",
            "Val. Loss: 3.064\n",
            "Epoch: 22\n",
            "Train Loss: 1.303\n",
            "Val. Loss: 3.040\n",
            "Epoch: 23\n",
            "Train Loss: 1.238\n",
            "Val. Loss: 3.091\n",
            "Epoch: 24\n",
            "Train Loss: 1.174\n",
            "Val. Loss: 2.975\n",
            "Epoch: 25\n",
            "Train Loss: 1.102\n",
            "Val. Loss: 3.005\n",
            "Epoch: 26\n",
            "Train Loss: 1.030 \n",
            "Val. Loss: 3.095\n",
            "Epoch: 27\n",
            "Train Loss: 1.987\n",
            "Val. Loss: 3.139\n",
            "Epoch: 28\n",
            "Train Loss: 1.917\n",
            "Val. Loss: 3.143\n",
            "Epoch: 29\n",
            "Train Loss: 1.889\n",
            "Val. Loss: 3.135\n",
            "Epoch: 30\n",
            "Train Loss: 1.796\n",
            "Val. Loss: 3.221\n",
            "Epoch: 31\n",
            "Train Loss: 1.748\n",
            "Val. Loss: 3.174\n",
            "Epoch: 32\n",
            "Train Loss: 1.700\n",
            "Val. Loss: 3.183\n",
            "Epoch: 33\n",
            "Train Loss: 1.672\n",
            "Val. Loss: 3.231\n",
            "Epoch: 34\n",
            "Train Loss: 1.624\n",
            "Val. Loss: 3.242\n",
            "Epoch: 35\n",
            "Train Loss: 1.574\n",
            "Val. Loss: 3.276\n",
            "Epoch: 36\n",
            "Train Loss: 1.511\n",
            "Val. Loss: 4.384\n",
            "Epoch: 37\n",
            "Train Loss: 1.476\n",
            "Val. Loss: 3.322\n",
            "Epoch: 38\n",
            "Train Loss: 1.438\n",
            "Val. Loss: 3.434\n",
            "Epoch: 39\n",
            "Train Loss: 1.394\n",
            "Val. Loss: 3.406\n",
            "Epoch: 40\n",
            "Train Loss: 1.365\n",
            "Val. Loss: 3.489\n",
            "Epoch: 41\n",
            "Train Loss: 1.305\n",
            "Val. Loss: 3.501\n",
            "Epoch: 42\n",
            "Train Loss: 1.274\n",
            "Val. Loss: 3.527\n",
            "Epoch: 43\n",
            "Train Loss: 1.245\n",
            "Val. Loss: 3.468\n",
            "Epoch: 44 \n",
            "Train Loss: 1.205\n",
            "Val. Loss: 3.565\n",
            "Epoch: 45\n",
            "Train Loss: 1.192\n",
            "Val. Loss: 3.577\n",
            "Epoch: 46\n",
            "Train Loss: 1.150\n",
            "Val. Loss: 3.651 \n",
            "Epoch: 47\n",
            "Train Loss: 1.095\n",
            "Val. Loss: 3.768\n",
            "Epoch: 48\n",
            "Train Loss: 1.102 \n",
            "Val. Loss: 3.725\n",
            "Epoch: 49\n",
            "Train Loss: 1.063\n",
            "Val. Loss: 2.774 \n",
            "Epoch: 50 \n",
            "Train Loss: 1.034\n",
            "Val. Loss: 2.800 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFxZA8ClGRT5",
        "outputId": "a815d03b-7f1f-486b-e1d3-ea77035dd798"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model-piyush.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 2.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jX_2zU9EVXW"
      },
      "source": [
        "pred = cross_val_predict(estimator=MemoryTagger(), X=words, y=tags, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJrvIeiEYJA",
        "outputId": "2994661c-dc13-4e00-d56f-178c05ae83f4"
      },
      "source": [
        "report = classification_report(y_pred=pred, y_true=tags)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "B-indications       0.68      0.37      0.48     18777\n",
            "I-indications       0.58      0.37      0.45     15770\n",
            "            O       0.99      1.00      0.99   1546622\n",
            "          nan       0.00      0.00      0.00         1\n",
            "\n",
            "     accuracy                           0.98   1581170\n",
            "    macro avg       0.56      0.43      0.48   1581170\n",
            " weighted avg       0.98      0.98      0.98   1581170\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}