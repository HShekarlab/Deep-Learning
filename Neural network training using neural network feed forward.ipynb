{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-End Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzRdKh6gpx-y"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V286w05oqmuH"
      },
      "source": [
        "file = open(\"train.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8drvu3dujPu"
      },
      "source": [
        "file = open(\"test.txt\", \"r\", encoding = \"utf8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFp1CVSTsBvJ"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isfile('glove.6B.50d.txt'):\n",
        "  !wget www.hlt.utdallas.edu/~ramon/images/glove.6B.50d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkL1Rl5rsETf"
      },
      "source": [
        "word2vec = {}\n",
        "with open('glove.6B.50d.txt') as f:\n",
        "  for line in f:\n",
        "    fields = line.split()\n",
        "    word2vec[fields[0]] = np.asarray(fields[1:])\n",
        "print('Loaded %d GloVe embeddings' % len(word2vec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFPZ77GhqpMB"
      },
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFxtsghqqtYy"
      },
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up3yf_KWqv1c"
      },
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEGNJyN1qzig"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1X3lPl0q5Sr"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npBXzRnyrCkh"
      },
      "source": [
        "***2***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2cKBdRYq-1K"
      },
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpbOMbAirN48"
      },
      "source": [
        "***3***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0-tfQCErPTf"
      },
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-2:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPLDQZaorIHD"
      },
      "source": [
        "***4***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPerDAvrJVi"
      },
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKUWR0JcrS3V"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scvbvuTfrVRA"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIc__jLSraPl"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "input_size = vocab_size\n",
        "hidden_size = 35\n",
        "output_size = vocab_size\n",
        "\n",
        "\n",
        "W = model.add_parameters((hidden_size, input_size))\n",
        "b = model.add_parameters(hidden_size)\n",
        "U = model.add_parameters((output_size, hidden_size))\n",
        "d = model.add_parameters(output_size)\n",
        "\n",
        "trainer = dy.SimpleSGDTrainer(model)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "model.add(Embedding(vocab_size,epochs=50, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWYMUVuRt-Uc"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "input_size = vocab_size\n",
        "hidden_size = 50\n",
        "output_size = vocab_size\n",
        "\n",
        "\n",
        "W = model.add_parameters((hidden_size, input_size))\n",
        "b = model.add_parameters(hidden_size)\n",
        "U = model.add_parameters((output_size, hidden_size))\n",
        "d = model.add_parameters(output_size)\n",
        "\n",
        "\n",
        "trainer = dy.SimpleSGDTrainer(model)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "model.add(Embedding(vocab_size,35, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUDiK_nhuA7x"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "input_size = vocab_size\n",
        "hidden_size = 100\n",
        "output_size = vocab_size\n",
        "\n",
        "\n",
        "W = model.add_parameters((hidden_size, input_size))\n",
        "b = model.add_parameters(hidden_size)\n",
        "U = model.add_parameters((output_size, hidden_size))\n",
        "d = model.add_parameters(output_size)\n",
        "\n",
        "trainer = dy.SimpleSGDTrainer(model)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "model.add(Embedding(vocab_size,epochs=100, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfWdzpYiuFM_"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "input_size = vocab_size\n",
        "hidden_size = 150\n",
        "output_size = vocab_size\n",
        "\n",
        "\n",
        "W = model.add_parameters((hidden_size, input_size))\n",
        "b = model.add_parameters(hidden_size)\n",
        "U = model.add_parameters((output_size, hidden_size))\n",
        "d = model.add_parameters(output_size)\n",
        "\n",
        "\n",
        "trainer = dy.SimpleSGDTrainer(model)\n",
        "\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "model.add(Embedding(vocab_size,150, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0pegseora6u"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BioBxW2drsKm"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.02))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VMpUbdirut4"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6RP60eprw6l"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFSJR6CrzO4"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.03))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkd0sACr1kS"
      },
      "source": [
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5JZRoBssKL0"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def perplexity(y_true, y_pred):\n",
        "  cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  perplexity = K.pow(2.0, cross_entropy)\n",
        "  return perplexity\n",
        "  \n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[perplexity, 'sparse_top_k_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy2kxQV-sRJ9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "perp = history_dict['perplexity']\n",
        "loss = history_dict['loss']\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-COzY1_cs5xI"
      },
      "source": [
        "***RNN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYv-JaBps5Rs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_data(path='datasets/data.txt'):\n",
        "    data = open(path, 'r').read()\n",
        "    chars = list(set(data))\n",
        "    return data, chars\n",
        "\n",
        "\n",
        "def convert_to_one_hot(data_, vocab):\n",
        "    data = np.zeros((len(data_), len(vocab)))\n",
        "    cnt = 0\n",
        "    for s in data_:\n",
        "        v = [0.0] * len(vocab)\n",
        "        v[vocab.index(s)] = 1.0\n",
        "        data[cnt, :] = v\n",
        "        cnt += 1\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_data(input):\n",
        "    # Load the data\n",
        "    data_ = \"\"\n",
        "    with open(input, 'r') as f:\n",
        "        data_ += f.read()\n",
        "    data_ = data_.lower()\n",
        "    # Convert to 1-hot coding\n",
        "    vocab = sorted(list(set(data_)))\n",
        "    data = convert_to_one_hot(data_, vocab)\n",
        "    return data, vocab"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}